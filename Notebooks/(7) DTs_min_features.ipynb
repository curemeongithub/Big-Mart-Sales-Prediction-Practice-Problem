{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from joblib import Memory\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "# Tree-based models\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = r\"C:\\Users\\malli\\Downloads\\Di-20250208T185422Z-001\\Di\\train_cleaned_FE.csv\"\n",
    "df = pd.read_csv(train_file)\n",
    "df = df.drop(columns=['MRP_by_Outlet_Type','Outlet_Location_Type', 'Outlet_Size', 'Item_Weight','Item_Fat_Content','Item_Visibility','Item_Type','Years_Since_Establishment','Mean_Sales_Outlet_Type','Avg_Sales_by_Location_Type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "categorical_cols = ['Item', 'Outlet', 'Outlet_Type']\n",
    "numerical_cols = ['Item_MRP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8523 entries, 0 to 8522\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Item               8523 non-null   object \n",
      " 1   Item_MRP           8523 non-null   float64\n",
      " 2   Outlet             8523 non-null   object \n",
      " 3   Outlet_Type        8523 non-null   object \n",
      " 4   Item_Outlet_Sales  8523 non-null   float64\n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 333.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(model, param_distributions, X, y, preprocessor, n_iter=10, cv=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Build a pipeline with the preprocessor and the model, and run RandomizedSearchCV.\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ], memory=memory)  # Cache intermediate steps\n",
    "    \n",
    "    rsearch = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring=rmse_scorer,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    rsearch.fit(X, y)\n",
    "    print(f\"Best params for {model.__class__.__name__}: {rsearch.best_params_}\")\n",
    "    return rsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(location='cache_dir', verbose=0)\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "], memory=memory)\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('yeo_johnson', PowerTransformer(method='yeo-johnson')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X = df.drop(columns=['Item_Outlet_Sales'])\n",
    "y = df['Item_Outlet_Sales']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(location='cache_dir', verbose=0)\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "], memory=memory)\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('yeo_johnson', PowerTransformer(method='yeo-johnson')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_params = {\n",
    "    'model__n_estimators': st.randint(50, 300),\n",
    "    'model__learning_rate': st.uniform(0.01, 0.2),\n",
    "    'model__max_depth': st.randint(3, 10),\n",
    "    'model__subsample': st.uniform(0.6, 0.4),\n",
    "    'model__min_samples_split': st.randint(2, 20),\n",
    "    'model__min_samples_leaf': st.randint(1, 10)\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'model__n_estimators': st.randint(50, 300),\n",
    "    'model__learning_rate': st.uniform(0.01, 0.2),\n",
    "    'model__max_depth': st.randint(3, 10),\n",
    "    'model__subsample': st.uniform(0.6, 0.4),\n",
    "    'model__min_child_weight': st.randint(1, 10)\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'model__n_estimators': st.randint(50, 300),\n",
    "    'model__learning_rate': st.uniform(0.01, 0.2),\n",
    "    'model__max_depth': st.randint(3, 10),\n",
    "    'model__subsample': st.uniform(0.6, 0.4),\n",
    "    'model__min_child_samples': st.randint(5, 50)\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'model__n_estimators': st.randint(50, 300),\n",
    "    'model__max_depth': st.randint(3, 15),\n",
    "    'model__min_samples_split': st.randint(2, 20),\n",
    "    'model__min_samples_leaf': st.randint(1, 20)\n",
    "}\n",
    "\n",
    "etr_params = {\n",
    "    'model__n_estimators': st.randint(50, 300),\n",
    "    'model__max_depth': st.randint(3, 15),\n",
    "    'model__min_samples_split': st.randint(2, 20),\n",
    "    'model__min_samples_leaf': st.randint(1, 20)\n",
    "}\n",
    "\n",
    "\n",
    "cat_params = {\n",
    "        'model__iterations': st.randint(50, 300),\n",
    "        'model__learning_rate': st.uniform(0.01, 0.2),\n",
    "        'model__depth': st.randint(3, 10)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning GradientBoostingRegressor...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for GradientBoostingRegressor: {'model__learning_rate': 0.021282315805420053, 'model__max_depth': 6, 'model__min_samples_leaf': 6, 'model__min_samples_split': 3, 'model__n_estimators': 241, 'model__subsample': 0.996884623716487}\n",
      "Tuning XGBRegressor...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best params for XGBRegressor: {'model__learning_rate': 0.1000998503939086, 'model__max_depth': 4, 'model__min_child_weight': 4, 'model__n_estimators': 138, 'model__subsample': 0.9862528132298237}\n",
      "Tuning LGBMRegressor...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malli\\anaconda3\\envs\\abhi_feb\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 283\n",
      "[LightGBM] [Info] Number of data points in the train set: 8523, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2181.288915\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best params for LGBMRegressor: {'model__learning_rate': 0.09638900372842316, 'model__max_depth': 3, 'model__min_child_samples': 31, 'model__n_estimators': 108, 'model__subsample': 0.7599443886861021}\n",
      "Tuning RandomForestRegressor...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best params for RandomForestRegressor: {'model__max_depth': 5, 'model__min_samples_leaf': 12, 'model__min_samples_split': 4, 'model__n_estimators': 278}\n",
      "Tuning ExtraTreesRegressor...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best params for ExtraTreesRegressor: {'model__max_depth': 7, 'model__min_samples_leaf': 7, 'model__min_samples_split': 12, 'model__n_estimators': 252}\n",
      "Tuning CatBoostRegressor...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best params for CatBoostRegressor: {'model__depth': 9, 'model__iterations': 171, 'model__learning_rate': 0.041198904067240534}\n"
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "\n",
    "print(\"Tuning GradientBoostingRegressor...\")\n",
    "best_models['gbr'] = tune_model(GradientBoostingRegressor(random_state=42), gbr_params, X, y, preprocessor)\n",
    "\n",
    "print(\"Tuning XGBRegressor...\")\n",
    "best_models['xgb'] = tune_model(XGBRegressor(objective='reg:squarederror', random_state=42), xgb_params, X, y, preprocessor)\n",
    "\n",
    "print(\"Tuning LGBMRegressor...\")\n",
    "best_models['lgbm'] = tune_model(LGBMRegressor(random_state=42), lgbm_params, X, y, preprocessor)\n",
    "\n",
    "print(\"Tuning RandomForestRegressor...\")\n",
    "best_models['rf'] = tune_model(RandomForestRegressor(random_state=42), rf_params, X, y, preprocessor)\n",
    "\n",
    "print(\"Tuning ExtraTreesRegressor...\")\n",
    "best_models['etr'] = tune_model(ExtraTreesRegressor(random_state=42), etr_params, X, y, preprocessor)\n",
    "\n",
    "print(\"Tuning CatBoostRegressor...\")\n",
    "best_models['cat'] = tune_model(CatBoostRegressor(verbose=0, random_state=42), cat_params, X, y, preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malli\\anaconda3\\envs\\abhi_feb\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to: C:\\Users\\malli\\Downloads\\Di-20250208T185422Z-001\\Di\\Decision_tree_models_minimum.csv\n"
     ]
    }
   ],
   "source": [
    "test_file = r\"C:\\Users\\malli\\Downloads\\Di-20250208T185422Z-001\\Di\\test_cleaned_FE.csv\"\n",
    "test_data = pd.read_csv(test_file)\n",
    "test_data = test_data.drop(columns=['MRP_by_Outlet_Type','Outlet_Location_Type', 'Outlet_Size', 'Item_Weight','Item_Fat_Content','Item_Visibility','Item_Type','Years_Since_Establishment','Mean_Sales_Outlet_Type','Avg_Sales_by_Location_Type'], errors='ignore')\n",
    "\n",
    "\n",
    "# Choosing the best models based on CV RMSE\n",
    "best_model_gbr = best_models['gbr']\n",
    "predictions_gbr = best_model_gbr.predict(test_data)\n",
    "test_data['Predicted_Item_Outlet_Sales_gbr'] = predictions_gbr\n",
    "test_data['Predicted_Item_Outlet_Sales_gbr_abs'] = np.abs(predictions_gbr)\n",
    "\n",
    "best_model_xgb = best_models['xgb']\n",
    "predictions_xgb = best_model_xgb.predict(test_data)\n",
    "test_data['Predicted_Item_Outlet_Sales_xgb'] = predictions_xgb\n",
    "test_data['Predicted_Item_Outlet_Sales_xgb_abs'] = np.abs(predictions_xgb)\n",
    "\n",
    "best_model_lgbm = best_models['lgbm']\n",
    "predictions_lgbm = best_model_lgbm.predict(test_data)\n",
    "test_data['Predicted_Item_Outlet_Sales_lgbm'] = predictions_lgbm\n",
    "test_data['Predicted_Item_Outlet_Sales_lgbm_abs'] = np.abs(predictions_lgbm)\n",
    "\n",
    "\n",
    "best_model_rf = best_models['rf']\n",
    "predictions_rf = best_model_rf.predict(test_data)\n",
    "test_data['Predicted_Item_Outlet_Sales_rf'] = predictions_rf\n",
    "test_data['Predicted_Item_Outlet_Sales_rf_abs'] = np.abs(predictions_rf)\n",
    "\n",
    "\n",
    "best_model_etr = best_models['etr']\n",
    "predictions_etr = best_model_etr.predict(test_data)\n",
    "test_data['Predicted_Item_Outlet_Sales_etr'] = predictions_etr\n",
    "test_data['Predicted_Item_Outlet_Sales_etr_abs'] = np.abs(predictions_etr)\n",
    "\n",
    "\n",
    "best_model_cat = best_models['cat']\n",
    "predictions_cat = best_model_cat.predict(test_data)\n",
    "test_data['Predicted_Item_Outlet_Sales_cat'] = predictions_cat\n",
    "test_data['Predicted_Item_Outlet_Sales_cat_abs'] = np.abs(predictions_cat)\n",
    "\n",
    "\n",
    "output_file = r\"C:\\Users\\malli\\Downloads\\Di-20250208T185422Z-001\\Di\\Decision_tree_models_minimum.csv\"\n",
    "test_data.to_csv(output_file, index=False)\n",
    "print(\"Predictions saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model_path = r\"C:\\Users\\malli\\Downloads\\Di-20250208T185422Z-001\\Di\\gbr_min.pkl\"\n",
    "with open(model_path,\"wb\") as f:\n",
    "    joblib.dump(best_model_gbr,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model_path = r\"C:\\Users\\malli\\Downloads\\Di-20250208T185422Z-001\\Di\\xgb_min.pkl\"\n",
    "with open(model_path,\"wb\") as f:\n",
    "    joblib.dump(best_model_xgb,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model_path = r\"C:\\Users\\malli\\Downloads\\Di-20250208T185422Z-001\\Di\\lgbm_min.pkl\"\n",
    "with open(model_path,\"wb\") as f:\n",
    "    joblib.dump(best_model_lgbm,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model_path = r\"C:\\Users\\malli\\Downloads\\Di-20250208T185422Z-001\\Di\\rf_min.pkl\"\n",
    "with open(model_path,\"wb\") as f:\n",
    "    joblib.dump(best_model_rf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model_path = r\"C:\\Users\\malli\\Downloads\\Di-20250208T185422Z-001\\Di\\etr_min.pkl\"\n",
    "with open(model_path,\"wb\") as f:\n",
    "    joblib.dump(best_model_etr,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model_path = r\"C:\\Users\\malli\\Downloads\\Di-20250208T185422Z-001\\Di\\cat_min.pkl\"\n",
    "with open(model_path,\"wb\") as f:\n",
    "    joblib.dump(best_model_cat,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abhi_feb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
